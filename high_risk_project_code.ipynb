{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "as5I5CeWu48L",
        "4M-2MaEpJMj3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Health Fact or Fiction? A Comparison of BERT-Based Models and LLMs on Detecting Health Misinformation About COVID-19 and Measles\n",
        "*High Risk Project, uaa99, Spring 2025*"
      ],
      "metadata": {
        "id": "xWV_cUt7QbKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 0: Dependencies\n",
        "\n",
        "For this project you will need to set your Google Gemini API key below."
      ],
      "metadata": {
        "id": "as5I5CeWu48L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import numpy as np\n",
        "import wandb\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "wandb.init(mode='disabled')\n",
        "client = genai.Client(api_key=\"YOUR API KEY HERE\")"
      ],
      "metadata": {
        "id": "nOxL5j9VQV1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Model Selection and Preparation\n",
        "\n",
        "We're going to be evaluating four models at this task:  BERT, Clinical-BERT, and BioMedBert and Gemini Flash."
      ],
      "metadata": {
        "id": "4M-2MaEpJMj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bertSeqClass = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3, force_download=True)\n",
        "bertSeqTokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "7t4yc4akQ2DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clinicalBert = AutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=3, force_download=True)\n",
        "clinicalBertTokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
      ],
      "metadata": {
        "id": "YOhW8yqLuDUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bioMedBert = AutoModelForSequenceClassification.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\", num_labels=3, force_download=True)\n",
        "bioMedBertTokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")"
      ],
      "metadata": {
        "id": "0_cCOQgdKXpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response_from_gemini(system_instruction, content):\n",
        "  return client.models.generate_content(\n",
        "      model=\"gemini-2.0-flash-lite\",\n",
        "      config=types.GenerateContentConfig(\n",
        "          system_instruction=system_instruction),\n",
        "      contents=content\n",
        "  )"
      ],
      "metadata": {
        "id": "Fkw0RxykKx6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_claim_with_llm(claim, tokenizer, model):\n",
        "    sys_message = '''\n",
        "    You are an AI Medical Assistant trained on a vast dataset of health information. Please evaluate the provided claim\n",
        "    and respond with the following determination:\n",
        "    0 - The claim is false\n",
        "    1 - The claim is true\n",
        "    2 - I am unable to make a determination\n",
        "\n",
        "    Please only respond with a 0, 1, or 2. Do not include any other text.\n",
        "    '''\n",
        "    # Create messages structured for the chat template\n",
        "    messages = [{\"role\": \"system\", \"content\": sys_message}, {\"role\": \"user\", \"content\": claim}]\n",
        "\n",
        "    # Applying chat template\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=False)\n",
        "\n",
        "    # Extract and return the generated text, removing the prompt\n",
        "    response_text = tokenizer.batch_decode(outputs)[0].strip()\n",
        "    print(response_text)\n",
        "    answer = response_text.split('<|im_start|>assistant')[-1].strip()\n",
        "    return answer"
      ],
      "metadata": {
        "id": "98wwpiD15rLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Data Loading and Preparation\n",
        "Let's begin by loading up the data we are going to need to train and evaluate our models. We are going to be using the Covid 19 News Rumors dataset from [A COVID-19 Rumor Dataset](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2021.644801/full), published in Frontiers in Psychology. And the Measles Rumors dataset created by me. Measles Rumors is publically available at this link, please download it and save it to a place where it's accessbile by this notebook."
      ],
      "metadata": {
        "id": "JGQjCfxIQvqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covid_claims = \"./news.csv\"\n",
        "df_covid = pd.read_csv(covid_claims, header=None, names=[\"id\", \"label\", \"text\", \"sentiment\"])\n",
        "df_covid.head()"
      ],
      "metadata": {
        "id": "wXqg62BiPVe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "measles_claims = \"./measles_claims.csv\"\n",
        "df_measles = pd.read_csv(measles_claims)\n",
        "print(df_measles['label'].value_counts())"
      ],
      "metadata": {
        "id": "tXHgXHudzrhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map the string labels to integer labels\n",
        "label_map = {'F': 0, 'T': 1, 'U': 2, 'U(Twitter)': 2}\n",
        "df_covid['label'] = df_covid['label'].map(lambda x: label_map.get(x))\n",
        "df_covid.head()"
      ],
      "metadata": {
        "id": "DJHpHqFmkfJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_covid['label'].value_counts())"
      ],
      "metadata": {
        "id": "RearQtDomFMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train test split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df_covid['text'].tolist(),\n",
        "    df_covid['label'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_texts_measles, val_labels_measles = df_measles['text'].tolist(), df_measles['label'].tolist()"
      ],
      "metadata": {
        "id": "AoQNOUefN3L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create custom pytorch dataset\n",
        "class MisinformationDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "h48IHP7POKeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Model Training\n",
        "\n",
        "Now let's train all three BERT-based models using the HuggingFace trainer API."
      ],
      "metadata": {
        "id": "bfv8OO-ROHud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_args(num_epochs):\n",
        "  return TrainingArguments(\n",
        "    output_dir=None,\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    learning_rate=2e-5,\n",
        "    lr_scheduler_type='linear',\n",
        "    report_to=\"none\"\n",
        "  )"
      ],
      "metadata": {
        "id": "8k301pucO4wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = predictions.argmax(axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted') # Use 'weighted' for multiclass\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }"
      ],
      "metadata": {
        "id": "a6SAprxgrUAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trainer(model, tokenizer, training_args):\n",
        "  if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "  train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "  val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "  train_dataset = MisinformationDataset(train_encodings, train_labels)\n",
        "  val_dataset = MisinformationDataset(val_encodings, val_labels)\n",
        "\n",
        "  return Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=val_dataset,\n",
        "      compute_metrics=compute_metrics,\n",
        "  )"
      ],
      "metadata": {
        "id": "bwZff5Y7rJP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init trainers\n",
        "training_args = get_training_args(15)\n",
        "bert_trainer = get_trainer(bertSeqClass, bertSeqTokenizer, training_args)\n",
        "clinical_bert_trainer = get_trainer(clinicalBert, clinicalBertTokenizer, training_args)\n",
        "bio_bert_trainer = get_trainer(bioMedBert, bioMedBertTokenizer, training_args)"
      ],
      "metadata": {
        "id": "HC4qBReJtdI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "trainers = {\n",
        "    \"BERT\": bert_trainer,\n",
        "    \"ClinicalBERT\": clinical_bert_trainer,\n",
        "    \"BioBERT\": bio_bert_trainer,\n",
        "}\n",
        "\n",
        "for name, trainer in trainers.items():\n",
        "  trainer.train()\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hBUQpTBLv_sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Evaluating Performance"
      ],
      "metadata": {
        "id": "H4rKVv5kO30E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results = bert_trainer.evaluate()\n",
        "clinical_bert_results = clinical_bert_trainer.evaluate()\n",
        "bio_bert_results = bio_bert_trainer.evaluate()"
      ],
      "metadata": {
        "id": "se8oD8aW-EuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bert Evaluation Results:\", json.dumps(bert_results, indent=4))"
      ],
      "metadata": {
        "id": "F9qzEmrpGd98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ClinicalBert Evaluation Results:\", json.dumps(clinical_bert_results, indent=4))"
      ],
      "metadata": {
        "id": "LVH5vRELGhOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BioBert Evaluation Results:\", json.dumps(bio_bert_results, indent=4))"
      ],
      "metadata": {
        "id": "chMNBFHQ3qde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_llm(val_texts, val_labels):\n",
        "    gemini_prompt = \"\"\"\n",
        "    You are a helpful medical assistant. Your job is to evaluate the factuality of a sentance about a health topic.\n",
        "    Please respond with one of the following options:\n",
        "    1. 0: The sentance is false, misleading, or inaccurate\n",
        "    2. 1: The sentance is true, factual, or correct\n",
        "    3. 2: You are unable to verify the factuality of the sentance.\n",
        "\n",
        "    Do not include any other text with the response.\n",
        "    \"\"\"\n",
        "    num_items = len(val_texts)\n",
        "    requests_sent = 0\n",
        "    start_time = time.time()\n",
        "    preds = []\n",
        "    requests_per_minute = 30\n",
        "\n",
        "    for i in range(len(val_texts)):\n",
        "        response = None\n",
        "        try:\n",
        "            claim = val_texts[i]\n",
        "            label = val_labels[i]\n",
        "            response = get_response_from_gemini(gemini_prompt, claim)\n",
        "            preds.append(response)\n",
        "        except Exception as e:\n",
        "            print(f\"Error for request {i+1}/{num_items}: {e}\")\n",
        "\n",
        "        requests_sent += 1\n",
        "\n",
        "        if requests_sent % requests_per_minute == 0:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            if elapsed_time < 60:\n",
        "                sleep_duration = 60 - elapsed_time\n",
        "                print(f\"Sent {requests_sent}/{num_items} requests. Sleeping for {sleep_duration:.2f} seconds to maintain rate limit of {requests_per_minute} per minute.\")\n",
        "                time.sleep(sleep_duration)\n",
        "            start_time = time.time()\n",
        "\n",
        "    print(f\"Finished sending {num_items} requests sequentially.\")\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "b3oLLaUW5e-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = evaluate_llm(val_texts, val_labels)"
      ],
      "metadata": {
        "id": "XLBsJj0b4ktI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_llm_metrics(predictions, val_labels):\n",
        "    predictions = [int(pred.text.rstrip('\\n')) for pred in predictions]\n",
        "    accuracy = accuracy_score(val_labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, predictions, average='weighted') # Use 'weighted' for multiclass\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "    }"
      ],
      "metadata": {
        "id": "mf-YUdO7R19i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_results = compute_llm_metrics(preds)"
      ],
      "metadata": {
        "id": "Me5F6RX2UnW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Gemini Evaluation Results:\", json.dumps(llm_results, indent=4))"
      ],
      "metadata": {
        "id": "GJEwLVA-VCty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_gemini_covid_preds(path):\n",
        "  lines = 90\n",
        "  with open(path, 'r') as file:\n",
        "    lines = [int(line.strip()) for line in file.readlines()]\n",
        "\n",
        "  return lines"
      ],
      "metadata": {
        "id": "UZm16ZfkEmKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_covid_eval_dataset(tokenizer):\n",
        "  val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "  val_dataset = MisinformationDataset(val_encodings, val_labels)\n",
        "\n",
        "  return val_dataset"
      ],
      "metadata": {
        "id": "O0t1TmgtAXH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preds_covid, _, _ = bert_trainer.predict(get_covid_eval_dataset(bertSeqTokenizer))\n",
        "clinical_bert_preds_covid, _, _ = clinical_bert_trainer.predict(get_covid_eval_dataset(clinicalBertTokenizer))\n",
        "bio_bert_preds_covid, _, _ = bio_bert_trainer.predict(get_covid_eval_dataset(bioMedBertTokenizer))"
      ],
      "metadata": {
        "id": "T4h9CqbGAar1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preds_covid = [prediction.argmax(axis=-1) for prediction in bert_preds_covid]\n",
        "clinical_bert_preds_covid = [prediction.argmax(axis=-1) for prediction in clinical_bert_preds_covid]\n",
        "bio_bert_preds_covid = [prediction.argmax(axis=-1) for prediction in bio_bert_preds_covid]"
      ],
      "metadata": {
        "id": "G-KyaX6XTdnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_predictions(predictions, labels, texts, is_llm=False):\n",
        "  errors = {\n",
        "      0: [],\n",
        "      1: [],\n",
        "      2: []\n",
        "  }\n",
        "  correct = {\n",
        "      0: [],\n",
        "      1: [],\n",
        "      2: []\n",
        "  }\n",
        "  for i in range(len(predictions)):\n",
        "    prediction = predictions[i]\n",
        "    label = labels[i]\n",
        "    claim = texts[i]\n",
        "    pred = prediction\n",
        "    if label == pred:\n",
        "      correct[label].append(claim)\n",
        "    else:\n",
        "      errors[label].append({\n",
        "          \"claim\": claim,\n",
        "          \"pred\": pred\n",
        "      })\n",
        "\n",
        "  return errors, correct"
      ],
      "metadata": {
        "id": "ehM6n6ncAlIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors_bert, correct_bert = evaluate_predictions(bert_preds_covid, val_labels, val_texts)\n",
        "errors_clinical_bert, correct_clinical_bert = evaluate_predictions(clinical_bert_preds_covid, val_labels, val_texts)\n",
        "errors_bio_bert, correct_bio_bert = evaluate_predictions(bio_bert_preds_covid, val_labels, val_texts)\n",
        "errors_gemini, correct_gemini = evaluate_predictions(gemini_covid_preds, val_labels, val_texts, True)"
      ],
      "metadata": {
        "id": "1uzyB6bZCfGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(val_texts))"
      ],
      "metadata": {
        "id": "Oz3YGVseZVlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(val_labels, bert_preds_covid))\n",
        "print(confusion_matrix(val_labels, clinical_bert_preds_covid))\n",
        "print(confusion_matrix(val_labels, bio_bert_preds_covid))\n",
        "print(confusion_matrix(val_labels, gemini_covid_preds))"
      ],
      "metadata": {
        "id": "dd0n6V5-C5rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_text_bert = set([item[\"claim\"] for item in errors_bert[1]])\n",
        "error_text_clinical_bert = set([item[\"claim\"] for item in errors_clinical_bert[1]])\n",
        "error_text_bio_bert = set([item[\"claim\"] for item in errors_bio_bert[1]])\n",
        "common_errors = error_text_clinical_bert & error_text_bert\n",
        "difference_errors = error_text_bio_bert - error_text_bert"
      ],
      "metadata": {
        "id": "zREQLr5iepVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(common_errors))"
      ],
      "metadata": {
        "id": "Fpb38Y6vinw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(difference_errors)[11])"
      ],
      "metadata": {
        "id": "-uzi6ewThyDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(errors_gemini[0])"
      ],
      "metadata": {
        "id": "sAPkzKE4j8B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(difference_errors)"
      ],
      "metadata": {
        "id": "7v1WKXWCjybq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(errors_clinical_bert[1])"
      ],
      "metadata": {
        "id": "VRgcbYJEfPVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Evaluating Performance on Claims about Measles"
      ],
      "metadata": {
        "id": "dCW-XRbS0rEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_measles_eval_dataset(tokenizer):\n",
        "  val_encodings = tokenizer(val_texts_measles, truncation=True, padding=True)\n",
        "  val_dataset = MisinformationDataset(val_encodings, val_labels_measles)\n",
        "\n",
        "  return val_dataset"
      ],
      "metadata": {
        "id": "7P0xUpmZ1EIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results_measles = bert_trainer.evaluate(eval_dataset=get_measles_eval_dataset(bertSeqTokenizer))\n",
        "clinical_bert_results_measles = clinical_bert_trainer.evaluate(eval_dataset=get_measles_eval_dataset(clinicalBertTokenizer))\n",
        "bio_bert_results_measles = bio_bert_trainer.evaluate(eval_dataset=get_measles_eval_dataset(bioMedBertTokenizer))"
      ],
      "metadata": {
        "id": "bXyyDsM50xK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bert Evaluation Results:\", json.dumps(bert_results_measles, indent=4))"
      ],
      "metadata": {
        "id": "VMLXtQb-1_m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ClinicalBert Evaluation Results:\", json.dumps(clinical_bert_results_measles, indent=4))"
      ],
      "metadata": {
        "id": "wLMiSJa_6ru0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"BioBert Evaluation Results:\", json.dumps(bio_bert_results_measles, indent=4))"
      ],
      "metadata": {
        "id": "TMCJ_5IE8_Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_measles = evaluate_llm(val_texts_measles, val_labels_measles)"
      ],
      "metadata": {
        "id": "RHJiQiWp9Fv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_results_measles = compute_llm_metrics(preds_measles, val_labels_measles)"
      ],
      "metadata": {
        "id": "ykc7YI5v94aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Gemini Evaluation Results:\", json.dumps(llm_results_measles, indent=4))"
      ],
      "metadata": {
        "id": "kltrrB2t-HrU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}